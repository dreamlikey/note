

### 用户空间与内核空间概念

现在的操作系统都是采用虚拟存储器，对于32位的操作系统而言，它的寻址空间（虚拟存储器空间）为4G

（2^32）。操作系统的内核独立于普通应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限，为了保证用户进程不能直接操作内核，保证内核的安全，操作系统将虚拟存储空间划分为两部分，一部分为内核空间，一部分为用户空间。**针对linux而言，将最高的1G字节供内核使用，称为内核空间，较低的3G字节供各用户进程使用，称为用户空间**。每个进程可以通过系统调用进入内核，因此，linux内核由系统内的所有进程共享。于是从具体进程角度来看，每个进程可以拥有4G字节的虚拟空间。

备注：64位操作系统，虚拟存储空间大小

**64位操作系统虚拟存储空间为256TB(2^48)为什么不是2^64字节，因为2^64字节太大完全用不上造成很大的空间浪费**



整个linux内部结构可以分为三个部分，硬件--》内核空间--》用户空间

1. 内核空间存放的是内核的代码和数据，用户空间存放的是用户进程的代码和数据，不管内核空间还是用户空间他们都处于虚拟空间中
2. Linux使用两级保护机制：0级供内核使用，3级供用户程序使用。



### linux 网络IO模型

进程无法直接操作IO设备的，其必须通过操作系统调用内核协助完成I/O动作，内核会为每个IO设备维护一个buffer。

#### IO过程

整个请求过程为：

**用户进程发起请求，内核接收到请求后，从IO设备获取数据到buffer，再将buffer中的数据copy到用户进程的地址空间，用户进程获取到数据后再响应客户端**

##### 两个阶段

以read举例，一个read操作经历两个阶段：

1. **等待数据准备**
2. **将数据从内核拷贝到用户进程中**

**对于socket流（网卡IO）而言：**

1. **第一步：通常涉及等待网络上的数据分组到达，然后被复制到内核缓冲区（buffer）**
2. **第二步：把数据从内核缓冲区复制到用户进程中**

**整个过程，数据从IO设备拷贝到内核需要时间，从内核拷贝到用户进程也需要时间，根据这两段时间的等待方式的不同划分为五种IO模型**



#### 五种网络IO模型

- 阻塞IO模型
- 非阻塞IO模型
- IO复用模型
- 信号驱动IO模型
- 异步IO模型

##### 阻塞IO(Blocking I/O)

###### 执行过程

linux中所有的socket默认都是阻塞blocking，一个读操作的大概流程：

当用户进程执行了recvfrom这个系统调用，进程就会阻塞，什么都不做，直到 数据准备好，并将数据拷贝到用户进程的内存，最后进程再处理数据。在等待数据和数据复制到用户进程的两个阶段，整个进程都被阻塞，不能处理别的io，直到数据复制到用户内存，进程才解除block阻塞状态。

阻塞IO大致流程图：

![阻塞IO执行过程](E:\wdq\note\netty\阻塞IO执行过程.jpg)

###### 特点

**Blocking io的特点就是在IO执行的两个阶段都阻塞了**



##### 非阻塞IO(non-Blocking IO)

在linux下可以通过设置socket使其变为non-blocking IO非阻塞IO

###### 执行过程

非阻塞IO操作大致流程：

**当用户进程调用recvform时，进程不会阻塞**，如果内核发现数据还没准备好，**立即返回ewouldblock错误**，进程不会等待数据而是立即得到一个返回结果，用户进程得到ewouldblock就知道数据还没准备好，于是就执行其它任务，其它任务执行完之后**进程再次执行recvform系统调用**，如果数据已经准备好，则拷贝数据到用户进程内存然后进行数据处理。

用户进程循环执行recvform系统调用称为轮询，应用程序不断轮询内核看数据是否准备好，这通常是**浪费CPU时间**（尤其是recvform调用很多的时候），这种IO模型只是偶尔会用到

非阻塞IO大致流程图：

![非阻塞IO执行过程](E:\wdq\note\netty\非阻塞IO执行过程.jpg)



###### 特点

不阻塞进程，但浪费CPU性能



##### IO多路复用

###### 执行过程

**IO复用的大致流程**

1. 有数据准备好通知用户进程
2. 用户进程调用recvform

IO多路复用其实就是调用select、poll、epoll函数，select(pool、epoll)好处是一个process可以监听多个网络连接（多个socket，可以理解为监听多个端口），当其中一个连接或多个连接的数据准备好后就通知用户用户进程，用户进程调用recvform，将数据复制到用户内存然后进行数据处理。



IO多路复用大致流程图：

![多路复用IO执行过程](E:\wdq\note\netty\多路复用IO执行过程.jpg)

###### 特点

1. **一个processer可以监听多个网络连接，虽然也会轮询网络连接但只有一次应用进程对内核的访问（非阻塞IO是用户进程不断轮询内核看数据是否准备好，很消耗CPU时间）**
2. **select、poll、epoll是阻塞函数，当用户进程调用了select，那么这个用户进程就会阻塞**

事实上当用户连接数少的时候，多路复用IO的性能还不如 阻塞IO + 多线程的方式，因为IO多路复用要使用两次系统调用（system call）select、recvform，而阻塞IO只使用了recvform，**但select/epoll的优势是在处理大量网络连接时的性能更高**



### 零拷贝

##### 场景

从一个文件中读出数据并将数据传到另一台服务器上？

1. `File.read(file, buf, len);`
2. `Socket.send(socket, buf, len);`

##### 执行过程（2次CPU拷贝）

![2-2次拷贝](E:\wdq\note\netty\2-2次拷贝.jpg)

读取本地文件（read）发送（write）到远端服务器要经历四次拷贝：

1、应用程序调用read()方法，切换上下文用户态->内核态，底层使用DMA（direct memory access）将磁盘数据拷贝到内核读缓冲区

2、将内核读缓冲区的数据拷贝到用户空间，切换上下文内核态-》用户态，如果有需要应用程序对数据进行处理（？这个动作谁主动发起的）

3、为了将文件内容发送到远端服务器，调用socket.send()方法，这里涉及到一次上下文切换用户态-》内核态，并且发生第三次拷贝，用户空间的数据拷贝到socket buffer套接字缓冲区

4、send()返回引发第四次上下文切换（内核态-》用户态），并进行第四次拷贝，DMA将socket buffer中的数据拷贝到协议引擎进行发送（NIC buffer 网卡缓冲区）

**整个过程1、4由DMA负责，2、3由CPU负责，4次拷贝（2次DMA拷贝，2次CPU拷贝），4次切换上下文**



##### 优化执行过程（1次CPU拷贝）

![2-1次拷贝](E:\wdq\note\netty\2-1次拷贝.jpg)

2、3两步由内核的read buffer 拷贝到 用户进程空间 再拷贝到 内核的socket buffer经历两次上下文切换和2次拷贝，**我们可以直接走read buffer将数据拷贝到socket buffer经历2次上下文切换，3次拷贝**

- 2次上下文切换
- 3次拷贝，2次DMA拷贝，1次CPU拷贝



##### 优化执行过程（零拷贝）

![2-零拷贝](E:\wdq\note\netty\2-零拷贝.png)

上文优化之后还是要经过一次CPU拷贝没有真正实现零拷贝，可以再次优化IO过程

在 Linux 内核 2.4 及后期版本中，针对套接字缓冲区描述符做了相应调整，**DMA自带了收集功能**，对于用户方面，用法还是一样，只是内部操作已经发生了改变：

- DMA将数据拷贝到内核读缓冲区
- 将内核读缓冲区的描述符（数据位置，长度）追加到套接字缓冲区，DMA引擎根据描述符直接把数据从内核读缓冲区传输到传输引擎从而避免了最后一次CPU拷贝

经历2次上下文切换，2次DMA拷贝，0次CPU拷贝



##### JAVA实现

"在Java中，FileChannel的transferTo() 方法可以实现这个过程，该方法将数据从文件通道传输到给定的可写字节通道， 上面的 `file.read()`和 `socket.send()` 调用动作可以替换为 **`transferTo()`** 调用

```java
public abstract long transferTo(long position, long count, WritableByteChannel target)
```



### select/epoll